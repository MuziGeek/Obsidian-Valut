
---
**2025-01-08**🌱上海: ⛅️  🌡️+5°C 🌬️→4km/h
## Redis集群的实现原理是什么?

### 为什么需要集群？

在讲Redis集群架构之前，我们先简单讲下Redis单实例的架构，从最开始的一主N从，到读写分离，再到Sentinel哨兵机制，单实例的Redis缓存足以应对大多数的使用场景，也能实现主从故障迁移。

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736257299045-1bf1ec96-3c96-4ead-85b5-379514055811.png)
- 单实例 Redis 缓存在某些场景下存在问题：

- 写并发：单实例读写分离能解决读操作负载均衡，但写操作全在 master 节点，海量数据高并发时，该节点易出现写瓶颈，压力上升。
- 海量数据的存储压力：单实例仅靠一台 Master 存储，面对海量数据难以应付，数据量大导致持久化成本高，可能阻塞服务器，降低服务请求成功率与服务稳定性。

- Redis 集群提供完善方案，解决了存储受单机限制和写操作无法负载均衡的问题。

### 什么是集群？

- Redis 3.0 加入集群模式，带来以下特性：

- 实现数据分布式存储：对数据分片，将不同数据存于不同 master 节点，解决海量数据存储问题。
- 去中心化思想：无中心节点，客户端视整个集群为一个整体，可连接任意节点操作，如同操作单一 Redis 实例，无需代理中间件。若操作的 key 未分配到该节点，Redis 返回转向指令，指向正确节点。
- 内置高可用机制：支持 N 个 master 节点，每个 master 节点可挂载多个 slave 节点。当 master 节点挂掉，集群会提升某个 slave 节点为新的 master 节点。

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736257666247-69e72a6e-4e01-413e-aac4-c05982718f09.png)

如上图所示，Redis集群可以看成多个主从架构组合起来的，每一个主从架构可以看成一个节点（其中，只有master节点具有处理请求的能力，slave节点主要是用于节点的高可用）

### 哈希槽算法

#### 什么是哈希槽算法？

分布式存储需考虑如何将数据拆分到不同 Redis 服务器，常见分区算法有 hash 算法、一致性 hash 算法。

- 普通 hash 算法：

- 计算方式：将 key 用 hash 算法计算后按节点数量取余，即 hash (key)% N。
- 优点：简单。
- 缺点：扩容或摘除节点时需重新计算映射关系，导致数据重新迁移。

- 一致性 hash 算法：

- 计算方式：为每个节点分配一个 token 构成哈希环，查找时先算 key 的 hash 值，再顺时针找第一个大于等于该哈希值的 token 节点。
- 优点：加入和删除节点仅影响相邻两个节点。
- 缺点：加减节点会造成部分数据无法命中，一般用于缓存，且适用于节点量大的情况，扩容通常增加一倍节点以保障数据负载均衡 。

Redis 集群采用哈希槽分区算法：

- 集群中有 16384 个哈希槽（范围 0 - 16383），不同哈希槽分布在不同 Redis 节点管理，每个节点负责部分哈希槽。
- 数据操作时，集群用 CRC16 算法对 key 计算并对 16384 取模（slot = CRC16 (key)%16383 ），得到的值就是 Key - Value 要放入的槽。
- 通过该值找到对应槽的 Redis 节点，进而在该节点进行存取操作。

使用哈希槽的好处就在于可以方便的**添加或者移除节点**，并且无论是添加删除或者修改某一个节点，都不会造成集群不可用的状态。当需要增加节点时，只需要把其他节点的某些哈希槽挪到新节点就可以了；当需要移除节点时，只需要把移除节点上的哈希槽挪到其他节点就行了；哈希槽数据分区算法具有以下几种特点：

- **解耦数据和节点之间的关系，简化了扩容和收缩难度；**
- **节点自身维护槽的映射关系，不需要客户端代理服务维护槽分区元数据**
- **支持节点、槽、键之间的映射查询，用于数据路由，在线伸缩等场景**

**槽的迁移与指派命令：CLUSTER ADDSLOTS 0 1 2 3 4 ... 5000**

#### Redis中哈希槽相关的数据结构

1. **clusterNode数据结构：**保存节点的当前状态，比如节点的创建时间，节点的名字，节点当前的配置纪元，节点的IP和地址，等等。

```
// 定义一个名为 clusterNode 的结构体，用于表示 Redis 集群中的节点
typedef struct clusterNode {
    // 节点对象的创建时间，以毫秒为单位的时间戳
    mstime_t ctime; /* Node object creation time. */
    
    // 节点名称，是一个十六进制字符串，长度为 REDIS_CLUSTER_NAMELEN（通常为 40 字节，SHA1 哈希值的长度）
    char name[REDIS_CLUSTER_NAMELEN]; /* Node name, hex string, sha1-size */
    
    // 节点的标志位，用于表示节点的各种状态，如是否是主节点、从节点、是否下线等，取值为 REDIS_NODE_... 系列的常量
    int flags;      /* REDIS_NODE_... */
    
    // 该节点观察到的最后一个配置纪元，用于集群配置的版本管理
    uint64_t configEpoch; /* Last configEpoch observed for this node */
    
    // 一个数组，用于表示该节点负责的哈希槽。每个字节表示 8 个哈希槽，REDIS_CLUSTER_SLOTS 通常为 16384，所以数组大小为 16384 / 8
    unsigned char slots[REDIS_CLUSTER_SLOTS/8]; /* slots handled by this node */
    
    // 该节点负责的哈希槽数量
    int numslots;   /* Number of slots handled by this node */
    
    // 如果该节点是主节点，这个字段表示它拥有的从节点数量
    int numslaves;  /* Number of slave nodes, if this is a master */
    
    // 一个指针数组，指向该主节点的所有从节点
    struct clusterNode **slaves; /* pointers to slave nodes */
    
    // 指向该从节点的主节点，如果该节点本身是主节点，则为 NULL
    struct clusterNode *slaveof; /* pointer to the master node */
    
    // 最近一次发送 PING 命令的时间，以毫秒为单位的时间戳
    mstime_t ping_sent;      /* Unix time we sent latest ping */
    
    // 最近一次接收到 PONG 响应的时间，以毫秒为单位的时间戳
    mstime_t pong_received;  /* Unix time we received the pong */
    
    // 当该节点被标记为 FAIL 状态的时间，以毫秒为单位的时间戳
    mstime_t fail_time;      /* Unix time when FAIL flag was set */
    
    // 最近一次为该主节点的某个从节点投票的时间，以毫秒为单位的时间戳
    mstime_t voted_time;     /* Last time we voted for a slave of this master */
    
    // 最近一次接收到该节点的复制偏移量的时间，以毫秒为单位的时间戳
    mstime_t repl_offset_time;  /* Unix time we received offset for this node */
    
    // 该节点最近已知的复制偏移量，用于主从复制的同步
    PORT_LONGLONG repl_offset;      /* Last known repl offset for this node. */
    
    // 该节点最近已知的 IP 地址，长度为 REDIS_IP_STR_LEN
    char ip[REDIS_IP_STR_LEN];  /* Latest known IP address of this node */
    
    // 该节点最近已知的端口号
    int port;                   /* Latest known port of this node */
    
    // 指向与该节点的 TCP/IP 连接的结构体
    clusterLink *link;          /* TCP/IP link with this node */
    
    // 一个链表，存储了所有报告该节点为失败的节点信息
    list *fail_reports;         /* List of nodes signaling this as failing */
} clusterNode;
```

2. **clusterState数据结构**：记录当前节点所认为的集群目前所处的状态。

```
// 定义一个名为 clusterState 的结构体，用于表示 Redis 集群的整体状态
typedef struct clusterState {
    // 指向代表本节点的 clusterNode 结构体指针
    clusterNode *myself;  /* This node */
    
    // 当前的集群配置纪元，用于标识集群配置的版本
    uint64_t currentEpoch;
    
    // 集群的当前状态，取值为 REDIS_CLUSTER_OK（集群正常）、REDIS_CLUSTER_FAIL（集群故障）等相关常量
    int state;            /* REDIS_CLUSTER_OK, REDIS_CLUSTER_FAIL,... */
    
    // 至少负责一个哈希槽的主节点数量
    int size;             /* Num of master nodes with at least one slot */
    
    // 一个字典，用于通过节点名称（字符串）查找对应的 clusterNode 结构体，方便快速定位节点
    dict *nodes;          /* Hash table of name -> clusterNode structures */
    
    // 一个字典，存储了在一段时间内不重新添加的节点，这些节点可能是出现问题或正在被处理的节点
    dict *nodes_black_list; /* Nodes we don't re-add for a few seconds. */
    
    // 一个数组，长度为 REDIS_CLUSTER_SLOTS（16384），每个元素指向一个 clusterNode 结构体，表示正在将某个哈希槽迁移到的目标节点
    clusterNode *migrating_slots_to[REDIS_CLUSTER_SLOTS];
    
    // 一个数组，长度为 REDIS_CLUSTER_SLOTS（16384），每个元素指向一个 clusterNode 结构体，表示正在从某个节点导入哈希槽
    clusterNode *importing_slots_from[REDIS_CLUSTER_SLOTS];
    
    // 一个数组，长度为 REDIS_CLUSTER_SLOTS（16384），每个元素指向一个 clusterNode 结构体，保存所有哈希槽位的分配情况
    clusterNode *slots[REDIS_CLUSTER_SLOTS];//保存所有槽位分配情况
    
    // 一个跳跃表，用于存储哈希槽到键的映射关系，方便根据哈希槽查找相关的键
    zskiplist *slots_to_keys;
    
    /* 以下字段用于从节点在选举中的状态 */
    // 上次或下次选举的时间，以毫秒为单位的时间戳
    mstime_t failover_auth_time; /* Time of previous or next election. */
    
    // 到目前为止收到的投票数
    int failover_auth_count;    /* Number of votes received so far. */
    
    // 表示是否已经请求过投票，为真则表示已经请求过
    int failover_auth_sent;     /* True if we already asked for votes. */
    
    // 当前从节点在本次选举请求中的排名
    int failover_auth_rank;     /* This slave rank for current auth request. */
    
    // 当前选举的纪元
    uint64_t failover_auth_epoch; /* Epoch of the current election. */
    
    // 表示从节点当前不能进行故障转移的原因，取值为 CANT_FAILOVER_* 系列的宏定义
    int cant_failover_reason;   /* Why a slave is currently not able to
                                   failover. See the CANT_FAILOVER_* macros. */
    
    /* 手动故障转移的通用状态 */
    // 手动故障转移的时间限制（以毫秒为单位的 Unix 时间戳），如果没有正在进行的手动故障转移，则为零
    mstime_t mf_end;            /* Manual failover time limit (ms unixtime).
                                   It is zero if there is no MF in progress. */
    
    /* 主节点手动故障转移的状态 */
    // 执行手动故障转移的从节点指针
    clusterNode *mf_slave;      /* Slave performing the manual failover. */
    
    /* 从节点手动故障转移的状态 */
    // 从节点开始手动故障转移所需的主节点偏移量，如果尚未收到则为零
    PORT_LONGLONG mf_master_offset; /* Master offset the slave needs to start MF
                                   or zero if stil not received. */
    
    // 如果非零，表示手动故障转移可以开始请求主节点投票
    int mf_can_start;           /* If non-zero signal that the manual failover
                                   can start requesting masters vote. */
    
    /* 以下字段用于主节点在选举中的状态 */
    // 上次授予投票的纪元
    uint64_t lastVoteEpoch;     /* Epoch of the last vote granted. */
    
    // 在 `clusterBeforeSleep()` 函数中需要完成的任务数量
    int todo_before_sleep; /* Things to do in clusterBeforeSleep(). */
    
    // 通过集群总线发送的消息数量
    PORT_LONGLONG stats_bus_messages_sent;  /* Num of msg sent via cluster bus. */
    
    // 通过集群总线接收的消息数量
    PORT_LONGLONG stats_bus_messages_received; /* Num of msg rcvd via cluster bus.*/
} clusterState;
```

#### 节点的槽指派信息

clusterNode数据结构的slots属性和numslot属性记录了节点负责处理那些槽：slots属性是一个二进制位数组(bit array)，这个数组的长度为16384/8=2048个字节，共包含16384个二进制位。Master节点用bit来标识对于某个槽自己是否拥有，时间复杂度为O(1)

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736258629816-a8a46622-33f7-483d-8ffd-3c75b3d3d0e1.png)

#### 集群所有槽的指派信息

当收到集群中其他节点发送的信息时，通过将节点槽的指派信息保存在本地的clusterState.slots数组里面，程序要检查槽i是否已经被指派，又或者取得负责处理槽i的节点，只需要访问clusterState.slots[i]的值即可，时间复杂度仅为O(1)

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736260249169-d5202a45-dc7c-4b15-96bd-c625d8aa6e2e.png)

ClusterState 中的 Slots 数组下标对应槽，槽信息对应 clusterNode（缓存节点），节点含实际 Redis 缓存服务的 IP 和 Port 信息。Redis Cluster 通讯机制确保各节点有其他节点和槽数据对应关系，因每个节点都有 ClusterState 记录所有槽与节点对应关系，所以客户端访问集群中任意节点都可路由到对应节点。

#### 集群的请求重定向

前面讲到，Redis集群在客户端层面没有采用代理，并且无论Redis 的客户端访问集群中的哪个节点都可以路由到对应的节点上，下面来看看 Redis 客户端是如何通过路由来调用缓存节点的：

1. MOVED请求

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736260745211-027bc700-25eb-4e2e-942b-c70f0001f1e9.png)

- Redis 客户端经计算找 “缓存节点 1” 操作数据。
- 因数据迁移等，对应 Slot 数据到 “缓存节点 2”，客户端无法从 “缓存节点 1” 获取。
- “缓存节点 1” 存集群节点信息，知数据在 “缓存节点 2”，发 MOVED 重定向请求。
- 客户端获 “缓存节点 2” 地址，继续访问并拿到数据。

2. ASK请求

上面的例子说明了，数据 Slot 从“缓存节点1”已经迁移到“缓存节点2”了，那么客户端可以直接找“缓存节点2”要数据。那么如果两个缓存节点正在做节点的数据迁移，此时客户端请求会如何处理呢？

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736260735931-9d67ef09-a019-4b5a-9553-d2d7233f92b8.png)

- Redis 客户端向 “缓存节点 1” 发出请求。
- 若 “缓存节点 1” 正向 “缓存节点 2” 迁移数据且未命中对应 Slot：

- “缓存节点 1” 会返回客户端一个 ASK 重定向请求，并告知 “缓存节点 2” 的地址。

- 客户端向 “缓存节点 2” 发送 Asking 命令，询问所需数据是否在 “缓存节点 2” 上。
- “缓存节点 2” 接到消息后，返回数据是否存在的结果。

3. 频繁重定向造成的网络开销的处理：smart客户端

1. **什么是smart客户端**

在大部分情况下，可能都会出现一次请求重定向才能找到正确的节点，这个重定向过程显然会增加集群的网络负担和单次请求耗时。所以大部分的客户端都是smart的。所谓 smart客户端，就是指客户端本地维护一份hashslot => node的映射表缓存，大部分情况下，直接走本地缓存就可以找到hashslot =>node，不需要通过节点进行moved重定向，

2. **JedisCluster的工作原理**

- JedisCluster 初始化时：

- 随机选择一个 node。
- 初始化 hashslot => node 映射表。
- 为每个节点创建一个 JedisPool 连接池。

- 每次基于 JedisCluster 执行操作时：

- 先在本地计算 key 的 hashslot。
- 在本地映射表找到对应的节点 node。

- 存在两种情况：

- 若该 node 仍持有此 hashslot，则操作正常进行。
- 若进行了 reshard 操作，hashslot 不在该 node 上，会返回 moved。

- 当 JedisCluster API 发现对应节点返回 moved 时：

- 利用节点返回的元数据，更新本地的 hashslot => node 映射表缓存。
- 重复上述步骤直至找到对应节点。

- 若重试超过 5 次：

- 报错，抛出 JedisClusterMaxRedirectionException。

3. **hashslot迁移和ask重定向**

若 hashslot 正在迁移，会向客户端返回 ask 重定向，客户端接收后重新定位到目标节点执行；因 ask 发生在迁移过程中，JedisCluster API 收到 ask 不会更新 hashslot 本地缓存。ASK 和 MOVED 虽都是对客户端的重定向控制，但有本质区别：ASK 重定向表明集群正在进行 slot 数据迁移，客户端无法知晓迁移完成时间，属于临时性重定向，客户端不更新 slots 缓存；MOVED 重定向说明键对应的槽已明确指定到新节点，客户端需更新 slots 缓存。

### Redis集群中节点的通信机制：goosip协议

Redis 集群的哈希槽算法解决数据存取问题，不同哈希槽分布在不同节点，各节点维护自身认知的集群状态，且集群采用去中心化架构。当集群状态如新节点加入、slot 迁移、节点宕机、从节点提升为主节点等发生变化时，需让其他节点尽快知晓，那么 Redis 如何处理以及不同节点间怎样通信以维护集群同步状态的呢？

在Redis集群中，不同的节点之间采用gossip协议进行通信，节点之间通讯的目的是为了维护节点之间的元数据信息。这些元数据就是每个节点包含哪些数据，是否出现故障，**通过gossip协议，达到最终数据的一致性。**

gossip协议，是基于病毒传播方式的节点或者进程之间信息交换的协议。原理就是在不同的节点间不断地通信交换信息，一段时间后，所有的节点就都有了整个集群的完整信息，并且所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，但只要这些节可以通过网络连通，最终他们的状态就会是一致的。Gossip协议最大的好处在于，即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。

**Redis集群中节点的通信过程如下：**

- 集群中每个节点都会单独开一个TCP通道，用于节点间彼此通信。
- 每个节点在固定周期内通过待定的规则选择几个节点发送ping消息
- 接收到ping消息的节点用pong消息作为响应

gossip 协议优点是分散了元数据更新的压力，缺点是元数据更新有延时致操作滞后，对服务器时间要求高，时间戳不准影响消息有效性，节点增多网络开销大且达最终一致性时间变长，官方推荐最大节点数约 1000。

redis cluster架构下的每个redis都要开放两个端口号，比如一个是6379，另一个就是加1w的端口16379。

- **6379端口号就是redis服务器入口。**
- **16379端口号是用来进行节点间通信的**，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用的是一种叫gossip 协议的二进制协议

#### 1. gossip协议的常见类型

gossip协议常见的消息类型包含： `ping`、`pong`、`meet`、`fail`等等。

- **meet**：主要用于通知新节点加入到集群中，通过「cluster meet ip port」命令，已有集群的节点会向新的节点发送邀请，加入现有集群。
- **ping**：用于交换节点的元数据。每个节点每秒会向集群中其他节点发送 ping 消息，消息中封装了自身节点状态还有其他部分节点的状态数据，也包括自身所管理的槽信息等等。

- 因为发送ping命令时要携带一些元数据，如果很频繁，可能会加重网络负担。因此，一般每个节点每秒会执行 **10 次 ping**，每次会选择 5 个最久没有通信的其它节点。
- 如果发现某个节点通信延时达到了 `cluster_node_timeout / 2`，那么立即发送 ping，避免数据交换延时过长导致信息严重滞后。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 `cluster_node_timeout` 可以调节，如果调得比较大，那么会降低 ping 的频率。
- 每次 ping，会带上自己节点的信息，还有就是带上 **1/10 其它节点的信息**，发送出去，进行交换。至少包含 **3 个其它节点**的信息，最多包含 **（总节点数 - 2）**个其它节点的信息。

- **pong**：ping和meet消息的响应，同样包含了自身节点的状态和集群元数据信息。
- **fail**：某个节点判断另一个节点 fail 之后，向集群所有节点广播该节点挂掉的消息，其他节点收到消息后标记已下线。

由于Redis集群的去中心化以及gossip通信机制，**Redis集群中的节点只能保证最终一致性**。例如当加入新节点时(meet)，**只有邀请节点和被邀请节点**知道这件事，其余节点要等待 ping 消息一层一层扩散。除了 **Fail 是立即全网通知**的，其他诸如新节点、节点重上线、从节点选举成为主节点、槽变化等，都需要等待被通知到，也就是Gossip协议是最终一致性的协议。

#### 2. **meet命令的实现**

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736263535927-d6f2aaef-9175-46a1-aaae-da15b61d212b.png)

1. 节点 A 为节点 B 创建 clusterNode 结构并添加到自身 clusterState.nodes 字典。
2. 节点 A 按 CLUSTER MEET 命令的 IP 和端口向节点 B 发 MEET 消息。
3. 节点 B 收到 MEET 消息后为节点 A 创建 clusterNode 结构并添加到自身 clusterState.nodes 字典。
4. 节点 B 向节点 A 返回 PONG 消息。
5. 节点 A 收到 PONG 消息，知晓节点 B 已接收 MEET 消息。
6. 节点 A 向节点 B 返回 PING 消息。
7. 节点 B 收到 PING 消息，知晓节点 A 已接收 PONG 消息，握手完成。
8. 节点 A 通过 Gossip 协议将节点 B 信息传播给其他节点，使其他节点与节点 B 握手，一段时间后节点 B 被集群所有节点认识。

### 集群的扩容与收缩

作为分布式部署的缓存节点总会遇到缓存扩容和缓存故障的问题。这就会导致缓存节点的上线和下线的问题。由于每个节点中保存着槽数据，因此当缓存节点数出现变动时，这些槽数据会根据对应的虚拟槽算法被迁移到其他的缓存节点上。所以对于redis集群，**集群伸缩主要在于槽和数据在节点之间移动**。

#### 1. 扩容

- 启动新节点
- 使用cluster meet命令将新节点加入到集群
- 迁移槽和数据：添加新节点后，需要将一些槽和数据从旧节点迁移到新节点

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736265686155-5d535f72-daa0-4bc2-a159-6aa4d9abea30.png)

如上图所示，集群中本来存在“缓存节点1”和“缓存节点2”，此时“缓存节点3”上线了并且加入到集群中。此时根据虚拟槽的算法，“缓存节点1”和“缓存节点2”中对应槽的数据会应该新节点的加入被迁移到“缓存节点3”上面。

新节点加入到集群的时候，作为孤儿节点是没有和其他节点进行通讯的。因此需要在集群中任意节点执行 cluster meet 命令让新节点加入进来。假设新节点是 192.168.1.1 5002，老节点是 192.168.1.1 5003，那么运行以下命令将新节点加入到集群中。

192.168.1.1 5003> cluster meet 192.168.1.1 5002

这个是由老节点发起的，有点老成员欢迎新成员加入的意思。新节点刚刚建立没有建立槽对应的数据，也就是说没有缓存任何数据。如果这个节点是主节点，需要对其进行槽数据的扩容；如果这个节点是从节点，就需要同步主节点上的数据。总之就是要同步数据。

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736265872417-99f3a30b-86e4-4823-94b1-e452d620feab.png)

如上图所示，由客户端发起节点之间的槽数据迁移，数据从源节点往目标节点迁移。

1. 客户端对目标节点发起准备导入槽数据的命令，让目标节点准备好导入槽数据。使用命令：cluster setslot {slot} importing {sourceNodeId}
2. 之后对源节点发起送命令，让源节点准备迁出对应的槽数据。使用命令：cluster setslot {slot} migrating {targetNodeId}
3. 此时源节点准备迁移数据了，在迁移之前把要迁移的数据获取出来。通过命令 cluster getkeysinslot {slot} {count}。Count 表示迁移的 Slot 的个数。
4. 然后在源节点上执行，migrate {targetIP} {targetPort} “” 0 {timeout} keys {keys} 命令，把获取的键通过流水线批量迁移到目标节点。
5. 重复 3 和 4 两步不断将数据迁移到目标节点。
6. 完成数据迁移到目标节点以后，通过 cluster setslot {slot} node {targetNodeId} 命令通知对应的槽被分配到目标节点，并且广播这个信息给全网的其他主节点，更新自身的槽节点对应表。

#### 2. 收缩

- 迁移槽。
- 忘记节点。通过命令 cluster forget {downNodeId} 通知其他的节点

![](https://cdn.nlark.com/yuque/0/2025/png/26566882/1736266160166-8161195d-4f28-4d4e-80d0-2e21e7740559.png)

为了安全删除节点，Redis集群只能下线没有负责槽的节点。因此如果要下线有负责槽的master节点，则需要先将它负责的槽迁移到其他节点。迁移的过程也与上线操作类似，不同的是下线的时候需要通知全网的其他节点忘记自己，此时通过命令 cluster forget {downNodeId} 通知其他的节点。

### 集群的故障检测与故障恢复机制

#### 1. 集群的故障检测

Redis集群的故障检测是基于gossip协议的，集群中的每个节点都会定期地向集群中的其他节点发送PING消息，以此交换各个节点状态信息，检测各个节点状态：在线状态、疑似下线状态PFAIL、已下线状态FAIL。

- 主观下线（pfail）：当节点A检测到与节点B的通讯时间超过了cluster-node-timeout 的时候，就会更新本地节点状态，把节点B更新为主观下线。

主观下线并不能代表某个节点真的下线了，有可能是节点A与节点B之间的网络断开了，但是其他的节点依旧可以和节点B进行通讯。

- 客观下线：由于集群内的节点会不断地与其他节点进行通讯，下线信息也会通过 Gossip 消息传遍所有节点，因此集群内的节点会不断收到下线报告。

当**半数以上的主节点**标记了节点B是主观下线时，便会触发客观下线的流程（该流程只针对主节点，如果是从节点就会忽略）。将主观下线的报告保存到本地的 ClusterNode 的结构**fail_reports**链表中，并且对主观下线报告的时效性进行检查，如果超过 **cluster-node-timeout*2** 的时间，就忽略这个报告，否则就记录报告内容，将其标记为**客观下线**。

接着向集群广播一条主节点B的**Fail 消息**，所有收到消息的节点都会标记节点B为客观下线。

#### 2. 集群的故障恢复

当故障节点下线后，如果是持有槽的主节点则需要在其从节点中找出一个替换它，从而保证高可用。此时下线主节点的所有从节点都担负着恢复义务，这些从节点会定时监测主节点是否进入客观下线状态，如果是，则触发故障恢复流程。故障恢复也就是选举一个节点充当新的master，选举的过程是基于Raft协议选举方式来实现的。

1. **从节点过滤**

检查每个slave节点与master节点断开连接的时间，如果超过了**cluster-node-timeout * cluster-slave-validity-factor**，那么就没有资格切换成master

2. **投票选举**

- **节点排序**： 对通过过滤条件的所有从节点进行排序，按照priority、offset、run id排序，排序越靠前的节点，越优先进行选举。

- priority的值越低，优先级越高
- offset越大，表示从master节点复制的数据越多，选举时间越靠前，优先进行选举
- 如果offset相同，run id越小，优先级越高

- **更新配置纪元**：每个主节点会去更新配置纪元（clusterNode.configEpoch），这个值是不断增加的整数。这个值记录了每个节点的版本和整个集群的版本。每当发生重要事情的时候（例如：出现新节点，从节点精选）都会增加全局的配置纪元并且赋给相关的主节点，用来记录这个事件。更新这个值目的是，保证所有主节点对这件“大事”保持一致，大家都统一成一个配置纪元，表示大家都知道这个“大事”了。

- **发起选举**：更新完配置纪元以后，从节点会向集群发起广播选举的消息（CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST），要求所有收到这条消息，并且具有投票权的主节点进行投票。每个从节点在一个纪元中只能发起一次选举。

- **选举投票**：如果一个主节点具有投票权，并且这个主节点尚未投票给其他从节点，那么主节点将向要求投票的从节点返回一条CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，表示这个主节点支持从节点成为新的主节点。每个参与选举的从节点都会接收CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK消息，并根据自己收到了多少条这种消息来统计自己获得了多少主节点的支持。

如果超过**(N/2 + 1)**数量的**master节点**都投票给了某个从节点，那么选举通过，这个从节点可以切换成master，如果在 **cluster-node-timeout*2** 的时间内从节点没有获得足够数量的票数，本次选举作废，**更新配置纪元**，并进行**第二轮选举**，直到选出新的主节点为止。

在节点排序领先的从节点通常会获得更多的票，因为它触发选举的时间更早一些，获得票的机会更大

3. **替换主节点**

当满足投票条件的从节点被选出来以后，会触发替换主节点的操作。删除原主节点负责的槽数据，把这些槽数据添加到自己节点上，并且广播让其他的节点都知道这件事情，新的主节点诞生了。

1. 被选中的从节点执行SLAVEOF NO ONE命令，使其成为新的主节点
2. 新的主节点会撤销所有对已下线主节点的槽指派，并将这些槽全部指派给自己
3. 新的主节点对集群进行广播PONG消息，告知其他节点已经成为新的主节点
4. 新的主节点开始接收和处理槽相关的请求

备注：如果集群中某个节点的master和slave节点都宕机了，那么集群就会进入fail状态，因为集群的slot映射不完整。如果集群超过半数以上的master挂掉，无论是否有slave，集群都会进入fail状态。

### Redis集群的搭建

Redis集群的搭建可以分为以下几个部分：

1、**启动节点**：将节点以**集群模式**启动，读取或者生成集群配置文件，此时节点是**独立**的。

2、**节点握手**：节点通过**gossip协议**通信，将独立的节点连成网络，**主要使用meet命令**。

3、**槽指派**：将16384个槽位分配给主节点，以达到分片保存数据库键值对的效果。

参考文章

[2W 字详解 Redis 集群环境搭建实践](https://juejin.cn/post/6922690589347545102#heading-1)

### Redis集群的运维

1、**数据迁移问题**

Redis集群可以进行节点的**动态扩容缩容**，这一过程目前还处于**半自动状态**，需要**人工介入**。在扩缩容的时候，需要进行**数据迁移**。而 Redis为了保证迁移的一致性，迁移所有操作都是**同步操作**，执行迁移时，**两端的 Redis**均会进入时长不等的**阻塞状态**，对于小Key，该时间可以忽略不计，但如果一旦Key的内存使用过大，严重的时候会直接触发集群内的**故障转移**，造成不必要的切换。

2、**带宽消耗问题**

Redis集群是**无中心节点的集群架构**，依靠**Gossip协议**协同自动化修复集群的状态，但goosip有**消息延时和消息冗余**的问题，在集群节点数量过多的时候，goosip协议通信会消耗大量的带宽，主要体现在以下几个方面：

- **消息发送频率**：跟`cluster-node-timeout`密切相关，当节点发现与其他节点的最后通信时间超过 `cluster-node-timeout/2`时会直接发送ping消息
- **消息数据量**：每个消息主要的数据占用包含：slots槽数组（2kb）和整个集群1/10的状态数据
- **节点部署的机器规模**：机器的带宽**上限是固定的**，因此相同规模的集群分布的机器越多，每台机器划分的节点越均匀，则整个集群内整体的可用带宽越高

**集群带宽消耗主要分为**：**读写命令消耗**+**Gossip消息消耗**，因此搭建Redis集群需要根据业务数据规模和消息通信成本做出合理规划：

- 在满足业务需求的情况下尽量**避免大集群**，同一个系统可以针对不同业务场景拆分使用若干个集群。
- 适度提供`cluster-node-timeout`**降低消息发送频率**，但是cluster-node-timeout还影响故障转移的速度，因此需要根据自身业务场景兼顾二者平衡
- 如果条件允许尽量均匀部署在更多机器上，**避免集中部署**。如果有60个节点的集群部署在3台机器上每台20个节点，这是机器的带宽消耗将非常严重

3、**Pub/Sub广播问题**：

集群模式下内部对**所有publish命令**都会向**所有节点**进行广播，加重带宽负担，所以集群应该**避免频繁使用Pub/sub功能**

4、**集群倾斜**：

集群倾斜是指不同节点之**间数据量和请求量**出现明显差异，这种情况将加大负载均衡和开发运维的难度。因此需要理解集群倾斜的原因

- **数据倾斜**：

- 节点和槽分配不均
- 不同槽对应键数量差异过大
- 集合对象包含大量元素
- 内存相关配置不一致

- **请求倾斜**：

- 合理设计键，热点**大集合对象做拆分**或者**使用hmget代替hgetall避免整体读取**

5、**集群读写分离**：

集群模式下**读写分离成本比较高**，**直接扩展主节点数量**来提高集群性能是更好的选择。

以上参考文章

[不懂Redis Cluster原理，我被同事diss了！](https://baijiahao.baidu.com/s?id=1663270958212268352&wfr=spider&for=pc)